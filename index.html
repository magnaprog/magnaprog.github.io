<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Capture The Flag variation of Pacman using reinforcement learning algorithms.">
  <meta property="og:title" content="Capture The Flag Pacman with Reinforcement Learning"/>
  <meta property="og:description" content="Explore the implementation of reinforcement learning algorithms in a Capture The Flag variation of Pacman."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="Capture The Flag Pacman with Reinforcement Learning">
  <meta name="twitter:description" content="Explore the implementation of reinforcement learning algorithms in a Capture The Flag variation of Pacman.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Reinforcement Learning, Pacman, Capture The Flag, Artificial Intelligence, Game Development">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Capture The Flag(CTF) Pacman with Reinforcement Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="video-wrapper header">
    <div class="bg-overlay"></div>
    <video playsinline autoplay muted loop>
      <source src="https://magnaprog.github.io/static/videos/approx_ql_training_3.mov" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="xtitle is-1 publication-title">Capture The Flag (CTF) Pacman with Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Yu-Hsin Weng</span>
              <span class="author-block">Kevin Lee</span>
              <span class="author-block">Varun Kumar</span>
              <span class="author-block">Siddarth Chalasani</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <small>Instructed by Prof. Demetri Terzopoulos</small><br><br>University of California, Los Angeles<br>
                <small>Department of Computer Science</small><br><br>CS275: Artificial Life for Computer Graphics and Vision, 2024
              </span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/report.pdf" target="_blank" class="external-link">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Report</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="static/pdfs/poster.pdf" target="_blank" class="external-link">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/magnaprog/minicontest2" target="_blank" class="external-link">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This project presents the development and implementation of a Capture The Flag (CTF) variant of Pacman, employing both traditional and machine learning (ML) approaches to enhance agent behavior. By integrating reinforcement learning (RL) algorithms such as Q-Learning, Approximate Q-Learning, and Proximal Policy Optimization (PPO), we have created intelligent agents capable of adaptive strategies in a dynamic, multi-agent environment. This interdisciplinary project bridges the fields of game development, artificial intelligence, and artificial life, providing valuable insights into cooperative and competitive behaviors in complex systems. The project demonstrates the practical application of advanced RL techniques in real-time decision-making scenarios. The results showcase the superior performance of our Approximate Q-Learning agents, achieving an 84% win rate against a baseline greedy agent team, highlighting the effectiveness of RL in enhancing game AI.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Rules</h2>
            <div class="content has-text-justified">
              <p>
                Each agent starts out as a ghost and becomes a Pacman after moving to the other side of the board. These Pacman must capture as many pellets as possible while avoiding the other team's ghosts. The team with the most points at the end of the game wins. A game lasts 1200 total moves or until all pellets are eaten.
              </p>
            </div>
          </div>
        </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/game_layout.png" alt="Layout"/>
        <h2 class="subtitle has-text-centered">
          Game Layout: We have two agents per team.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/frightened.png" alt="Power Pellet"/>
        <h2 class="subtitle has-text-centered">
          Power Pellets: These give the Pacman the ability to eat the other team's ghosts.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Q-Learning Algorithm</h2>
          <div class="content has-text-justified">
            <p>
              The Q-Learning algorithm is a model-free reinforcement learning technique used to find the optimal action-selection policy. It updates the Q-values of state-action pairs based on the received rewards, guiding the agent to learn effective strategies through exploration and exploitation.
            </p>
            <p>
              The Q-value update rule is given by:
              \[
              Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
              \]
              where \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, \( r \) is the reward, \( s \) is the current state, \( s' \) is the next state, \( a \) is the current action, and \( a' \) is the next action.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Approximate Q-Learning Algorithm</h2>
          <div class="content has-text-justified">
            <p>
              Approximate Q-Learning extends Q-Learning by using function approximation to handle large state spaces. This approach leverages features of the states to approximate the Q-values, making it suitable for complex environments like the Capture The Flag Pacman game.
            </p>
            <p>
              The Q-value update rule with function approximation is:
              \[
              Q(s, a; \theta) \leftarrow Q(s, a; \theta) + \alpha \left[ r + \gamma \max_{a'} Q(s', a'; \theta) - Q(s, a; \theta) \right]
              \]
              where \( \theta \) represents the parameters of the function approximator.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Proximal Policy Optimization (PPO) Algorithm</h2>
          <div class="content has-text-justified">
            <p>
              Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that optimizes the policy directly. It strikes a balance between exploration and exploitation by maintaining a probability ratio, which ensures stable and efficient learning. PPO is known for its simplicity and effectiveness in various RL tasks.
            </p>
            <p>
              The PPO objective function is:
              \[
              L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
              \]
              where \( r_t(\theta) \) is the probability ratio, \( \hat{A}_t \) is the advantage estimate, and \( \epsilon \) is a hyperparameter that controls the clipping range.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results: Red Team (Greedy Agents) vs. Blue Team (Experimental Agents)<br/>[Videos]</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%">
            <source src="static/videos/qlearning.mov" type="video/mp4">
          </video>
          <h1 class="subtitle has-text-centered">
            Q-Learning: As we see here the Blue Agents ends up taking a lot of random moves because many of the states are unique and thus they are not even in the Q-table at all.
          </h1>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%">
            <source src="static/videos/ppo.mov" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            PPO: Both of the Blue Agents fail to learn anything substantial.
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%">
            <source src="static/videos/approx_qlearning.mov" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Approximate Q-Learning: The Blue Agents are able to learn a offensive/defensive strategy that allows them to beat the red team.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Training of the Approximate Q-Learning Model<br/>[Videos]</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%">
            <source src="static/videos/approx_ql_training_1.mov" type="video/mp4">
          </video>
          <h1 class="subtitle has-text-centered">
            Episode 10: The Blue Agents are taking random moves because they are still exploring the environment, but we see an offensive strategy forming. 
          </h1>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%">
            <source src="static/videos/approx_ql_training_2.mov" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Episode 100: The Blue Agents are starting to learn a really good offense/defense strategy that allows them to beat the team of greedy agents.
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%">
            <source src="static/videos/approx_ql_training_3.mov" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Episode 400: The Blue Agents have fine-tuned their strategy that allows them to beat the red team very consistently.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Project Report</h2>
      <iframe src="static/pdfs/report.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Project Poster</h2>
      <iframe src="static/pdfs/poster.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      &copy; 2024 Yu-Hsin Weng, Kevin Lee, Varun Kumar, Siddarth Chalasani. All rights reserved.
    </p>
    <p>
      Instructed by Prof. Demetri Terzopoulos, UCLA.
    </p>
  </div>
</footer>

</body>
</html>
